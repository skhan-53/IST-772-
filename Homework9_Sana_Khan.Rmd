---
title: "Homework 9"
author: "Sana Khan"
date: "2022-12-04"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
install.packages("/Users/gozi/Downloads/BaylorEdPsych_0.5.tar.gz", 
                 repos = NULL, type = "source")



```

1.The built‐in data sets of R include one called “mtcars,” which stands for Motor Trend cars. Motor Trend was the name of an automotive magazine and this data set contains information on cars from the 1970s. Use “?mtcars” to display help about the data set. The data set includes a dichotomous variable called vs, which is coded as 0 for an engine with cylinders in a v‐shape and 1 for so called “straight” engines. Use logistic regression to predict vs, using two metric variables in the data set, gear (number of forward gears) and hp (horsepower). Interpret the resulting null hypothesis significance tests.

```{r cars}
?mtcars

par(mfrow=c(1,2))
boxplot(gear ~ vs, data=mtcars, main="Top Gear vs. Engine Config.", col="orange") 
boxplot(hp ~ vs, data=mtcars, main="Horsepower vs. Engine Config.", col="skyblue") 

glmOut <- glm(formula = vs ~ gear + hp, family = binomial(), data = mtcars)
summary(glmOut)
plot(glmOut)

exp(coef(glmOut))
```
Looking at the p-value for the intercept,.061, we are fail to reject the null hypothsis since it's larger than our alpha value. The p-value for our first variable, gear is .391 so we also fail to reject the null hypothesis. The last variable, horsepower is statistically signifcant with a p-value of .014.


5.As noted in the chapter, the BaylorEdPsych add‐in package contains a procedure for generating pseudo‐R‐squared values from the output of the glm() procedure. Use the results of Exercise 1 to generate, report, and interpret a Nagelkerke pseudo‐R‐squared value.

```{r pressure, echo=FALSE}
library(BaylorEdPsych)
PseudoR2(glmOut)
```
 Nagelkerke PseudoR2 allows us to loosely interpret the proportion of variance in the dependent variable accounted by the independent variables. In this case, .779 is the variance between engine shape, gear and horsepower 
 




6.Continue the analysis of the Chile data set described in this chapter. The data set is in the “car” package, so you will have to install.packages() and library() that package first, and then use the data(Chile) command to get access to the data set. Pay close attention to the transformations needed to isolate cases with the Yes and No votes as shown in this chapter. Add a new predictor, statusquo, into the model and remove the income variable. Your new model specification should be vote ~ age + statusquo. The statusquo variable is a rating that each respondent gave indicating whether they preferred change or maintaining the status quo. Conduct general linear model and Bayesian analysis on this model and report and interpret all relevant results. Compare the AIC from this model to the AIC from the model that was developed in the chapter (using income and age as predictors).
```{r}
library(car)

data(Chile)

# transform Chile dataset
ChileN=Chile[Chile$vote=='N',] # isolate yes votes
ChileY=Chile[Chile$vote=='Y',] # isolate no votes
ChileYN=rbind(ChileN, ChileY) # combine both yes and no
ChileYN=ChileYN[complete.cases(ChileYN),] # get rid of missing data
ChileYN$vote=factor(ChileYN$vote, levels=c("N", "Y")) # simplify the factor
str(ChileYN)


chileGLM=glm(vote ~ age + statusquo, family=binomial(), data=ChileYN)
summary(chileGLM)

exp(coef(chileGLM))
library(MCMCpack)

ChileYN$vote=as.numeric(ChileYN$vote)-1 # adjust outcome variable
ChileBayes=MCMClogit(formula=vote~ age + statusquo, data=ChileYN)
summary(ChileBayes)
plot(ChileBayes)

```
The Bayes model shows that Statusquo is a better predictor of how a voter will vote. The p-value for statusquo is significant, where as the p-value for the intercept and age is not statistically significant. The HDI for statusquo also does not overlap with 0, which means we can reject tje null hypothesis for statusquo, but fail to rject for the intercept and age. The AIC for this model is 740.52 whereas the model in thebook is  2332. This model has a better  AIC, which means it has better error reduction.e

7.Bonus R code question: Develop your own custom function that will take the posterior distribution of a coefficient from the output object from an MCMClogit() analysis and automatically create a histogram of the posterior distributions of the coefficient in terms of regular odds (instead of log‐odds). Make sure to mark vertical lines on the histogram indicating the boundaries of the 95% HDI.

```{r}
#library(ChileBayes)
OddsHistogram <- function(mcmc_out, seq){
  logOdds <- as.matrix(mcmc_out[,3])
  odds <- apply(logOdds,1,exp)
  hist(odds, col="skyblue", 
       main="Histogram of Statusquo Odds - Bayesian Analysis", 
       xlab='statusquo odds')
  abline(v=quantile(odds,c(0.025)),col='black')
  abline(v=quantile(odds,c(0.975)),col='black')
}
OddsHistogram(ChileBayes, 3)
```


